{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Lecture 3-COM4509-6509.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-ujgf-lVGb6j","colab_type":"text"},"source":["# Regression \n","\n","## Machine Learning and Adaptive Intelligence\n","\n","### Mauricio Álvarez \n","\n","### Based on slides by Neil D. Lawrence"]},{"cell_type":"code","metadata":{"id":"iOr6Xy9EGb6p","colab_type":"code","colab":{}},"source":["import pods\n","from matplotlib import pyplot as plt\n","import numpy as np\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzMv7_rNGb62","colab_type":"text"},"source":["### Review\n","- Last time: Looked at objective functions for movie recommendation.\n","- Minimized sum of squares objective by steepest descent and stochastic gradients.\n","- This time: explore least squares for regression."]},{"cell_type":"markdown","metadata":{"id":"ghZ0B0mMGb65","colab_type":"text"},"source":["### Regression Examples\n","\n","-   Predict a real value, $y_i$ given some inputs\n","    $x_i$.\n","\n","-   Predict quality of meat given spectral measurements (Tecator data).\n","\n","-   Radiocarbon dating, the C14 calibration curve: predict age given\n","    quantity of C14 isotope.\n","\n","-   Predict quality of different Go or Backgammon moves given expert\n","    rated training data.\n"]},{"cell_type":"markdown","metadata":{"id":"sR87-70cGb67","colab_type":"text"},"source":["### Olympic 100m Data\n","\n","-  Gold medal times for Olympic 100 m runners since 1896.\n","\n","![image](./diagrams/100m_final_start.jpg)\n","Image from Wikimedia Commons <http://bit.ly/191adDC>"]},{"cell_type":"markdown","metadata":{"id":"jt7j0FcFGb69","colab_type":"text"},"source":["### Olympic 100m Data\n","\n","\n","<img src=\"diagrams/male100.jpeg\" width=\"500\" height=\"40\" align=center>"]},{"cell_type":"markdown","metadata":{"id":"eOf24GvHGb6_","colab_type":"text"},"source":["### Olympic Marathon Data\n","\n","-   Gold medal times for Olympic Marathon since 1896.\n","\n","-   Marathons before 1924 didn’t have a standardised distance.\n","\n","-   Present results using pace per km.\n","\n","-   In 1904 Marathon was badly organised leading to very slow times.\n","\n","<img src=\"diagrams/Eliud_Kipchoge.jpg\" width=\"300\" height=\"40\" align=center>\n","\n","Image from Wikimedia Commons [Eliud Kipchoge](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)\n"]},{"cell_type":"markdown","metadata":{"id":"Dx7ARKD0Gb7A","colab_type":"text"},"source":["### Olympic Marathon Data\n"]},{"cell_type":"code","metadata":{"id":"tmxdubOkGb7B","colab_type":"code","outputId":"09de696a-0567-4fca-955b-2e237440e38e","executionInfo":{"status":"ok","timestamp":1571007007753,"user_tz":-60,"elapsed":755,"user":{"displayName":"Ming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD4AkcM4sv1rI1zSX3xC_6PkLOatOKjqPP4Ack=s64","userId":"11875485666463178947"}},"colab":{"base_uri":"https://localhost:8080/","height":450}},"source":["data = pods.datasets.olympic_marathon_men()\n","# Adding 2016 time\n","np.append(data['X'], 2016)\n","np.append(data['Y'], (2*60+8+(44/60))/42.195) # 2:08:44\n","f, ax = plt.subplots(figsize=(7,7))\n","ax.plot(data['X'], data['Y'], 'ro',markersize=10)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f171ca68780>]"]},"metadata":{"tags":[]},"execution_count":10},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAa8AAAGfCAYAAADoEV2sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSxi/YrIAAAAASUVORK5CYII=\n","text/plain":["<Figure size 504x504 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nZRA2NPoGb7G","colab_type":"text"},"source":["### What is Machine Learning?\n","\n","$$ \\text{data} + \\text{model} = \\text{prediction}$$\n","\n","-   $\\text{data}$ : observations, could be actively or passively\n","    acquired (meta-data).\n","\n","-   $\\text{model}$ : assumptions, based on previous experience (other data!\n","    transfer learning etc), or beliefs about the regularities of\n","    the universe. Inductive bias.\n","\n","-   $\\text{prediction}$ : an action to be taken or a categorization or a\n","    quality score.\n"]},{"cell_type":"markdown","metadata":{"id":"hjO84BFAGb7H","colab_type":"text"},"source":["### Regression: Linear Releationship\n","\n","$$y_i = m x_i + c$$\n","\n","-   $y_i$ : winning time/pace.\n","\n","-   $x_i$ : year of Olympics.\n","\n","-   $m$ : rate of improvement over time.\n","\n","-   $c$ : winning time at year 0."]},{"cell_type":"markdown","metadata":{"id":"I9ZHzbpZGb7J","colab_type":"text"},"source":["# Overdetermined System\n"]},{"cell_type":"markdown","metadata":{"id":"StUlXSJBGb7K","colab_type":"text"},"source":["<img src=\"diagrams/two_points.jpeg\" width=\"500\" height=\"40\" align=center>"]},{"cell_type":"markdown","metadata":{"id":"gJ87ZOiEGb7L","colab_type":"text"},"source":["# $y = mx + c$\n","\n","point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n","point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n","\n","*We know how to solve this system of two equations and two unknowns*"]},{"cell_type":"markdown","metadata":{"id":"r2cguOveGb7L","colab_type":"text"},"source":["<img src=\"diagrams/two_points_plus_line.jpeg\" width=\"500\" height=\"40\" align=center>"]},{"cell_type":"markdown","metadata":{"id":"K9-c12HrGb7M","colab_type":"text"},"source":["## We now observe a new point"]},{"cell_type":"markdown","metadata":{"id":"jgPnbJZmGb7N","colab_type":"text"},"source":["<img src=\"diagrams/three_points.jpeg\" width=\"500\" height=\"40\" align=center>"]},{"cell_type":"markdown","metadata":{"id":"8BFeo-FwGb7N","colab_type":"text"},"source":["# $y = mx + c$\n","\n","point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n","point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n","point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$\n","\n","*Overdetermined system (more equations than unknowns): three equations and two unknowns* "]},{"cell_type":"markdown","metadata":{"id":"L9u66dlmGb7O","colab_type":"text"},"source":["<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=50%>"]},{"cell_type":"markdown","metadata":{"id":"Dn8TkZHpGb7P","colab_type":"text"},"source":["# $y = mx + c + \\epsilon$\n","\n","point 1: $x = 1$, $y=3$ \n","$$3 = m + c + \\epsilon_1$$ \n","\n","point 2: $x = 3$, $y=1$ \n","$$1 = 3m + c + \\epsilon_2$$ \n","\n","point 3: $x = 2$, $y=2.5$ \n","$$2.5 = 2m + c + \\epsilon_3$$"]},{"cell_type":"markdown","metadata":{"id":"Kj9y-fC7Gb7P","colab_type":"text"},"source":["### The Gaussian Density\n","\n","Perhaps the most common probability density.\n","\n","\\begin{align*}\n","p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n","                    & \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n","\\end{align*}\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iNlSLs1MGb7Q","colab_type":"text"},"source":["### Gaussian Density\n","\n","![](./diagrams/gaussian_of_height.svg)\n","\n","The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n","  0.0225$. Mean shown as red line. It could represent the heights of a population of\n","  students."]},{"cell_type":"markdown","metadata":{"id":"mHjim9wrGb7Q","colab_type":"text"},"source":["### Gaussian Density\n","$$\n","\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n","$$\n","$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"]},{"cell_type":"markdown","metadata":{"id":"b-ykQmtBGb7R","colab_type":"text"},"source":["\n","### Two Important Gaussian Properties\n","\n","**Sum of Gaussian**\n","\n","-   Sum of Gaussian variables is also Gaussian.\n","    \n","    $$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2_i)$$ \n","    \n","    And the sum is distributed as\n","    \n","    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n","    \n","    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n","    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"]},{"cell_type":"markdown","metadata":{"id":"HSC0AVMTGb7S","colab_type":"text"},"source":["### Two Important Gaussian Properties\n","\n","**Scaling a Gaussian**\n","\n","-   Scaling a Gaussian leads to a Gaussian.\n","    \n","    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n","    \n","    And the scaled density is distributed as\n","    \n","    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"]},{"cell_type":"markdown","metadata":{"id":"irK-656BGb7S","colab_type":"text"},"source":["## Laplace's Idea\n","\n","### A Probabilistic Process\n","\n","-   Set the mean of Gaussian to be a function.\n","    \n","    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}\n","    {2\\sigma^2}\\right).$$\n","\n","\n","-   This gives us a ‘noisy function’.\n"]},{"cell_type":"markdown","metadata":{"id":"WSWoau_1Gb7T","colab_type":"text"},"source":["### Height as a Function of Weight\n","\n","-   In the standard Gaussian, parametized by mean and variance.\n","\n","-   Make the mean a linear function of an *input*.\n","\n","-   This leads to a regression model. \n","\n","    \\begin{align*}\n","               y_i=    &  f\\left(x_i\\right)+\\epsilon_i,\\\\\n","       \\epsilon_i \\sim &  \\mathcal{N}(0, \\sigma^2).\n","     \\end{align*}\n","        \n","-   Assume $y_i$ is height and $x_i$ is weight."]},{"cell_type":"markdown","metadata":{"id":"aP86Ru_9Gb7U","colab_type":"text"},"source":["### Data Point Likelihood\n","\n","-   Likelihood of an individual data point\n","    $$p\\left(y_i|x_i,m,c\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n","    \n","\n","-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n","    variance $\\sigma^2$."]},{"cell_type":"markdown","metadata":{"id":"Nd5y9oXPGb7U","colab_type":"text"},"source":["### Data Set Likelihood\n","\n","-   If the noise, $\\epsilon_i$ is sampled independently for each\n","    data point.\n","\n","-   Each data point is independent (given $m$ and $c$).\n","\n","-   For independent variables:\n","    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n","    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n p(y_i|x_i, m, c)$$\n","    "]},{"cell_type":"markdown","metadata":{"id":"KZkrEhbIGb7V","colab_type":"text"},"source":["### For Gaussian \n","\n","- i.i.d. assumption\n","\\begin{align*}\n","                   p(\\mathbf{y}) &= \\prod_{i=1}^n p(y_i)\\\\\n","   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\prod_{i=1}^n p(y_i|x_i, m, c)\\\\\n","   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-  \n","                                     c\\right)^{2}}{2\\sigma^2}\\right)\\\\\n","   p(\\mathbf{y}|\\mathbf{x}, m, c)&= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n","                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).\n"," \\end{align*}"]},{"cell_type":"markdown","metadata":{"id":"osq25s8RGb7W","colab_type":"text"},"source":["### Log Likelihood Function\n","\n","- We can use the likelihood function, $p(\\mathbf{y}|\\mathbf{x}, m, c)$, \n","\n","     $$p(\\mathbf{y}|\\mathbf{x}, m, c)= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n","                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right),$$\n","\n"," to estimate the parameters $m$ and $c$.\n","\n","- In practice, we prefer to work with the log likelihood:\n","    \n","    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-    \n","        c\\right)^{2}}{2\\sigma^2}.$$"]},{"cell_type":"markdown","metadata":{"id":"yQcETYGPGb7W","colab_type":"text"},"source":["### Consistency of Maximum Likelihood\n","\n","\n","-   If data was really generated according to the probability we specified, the correct parameters will be recovered   in the limit as $n \\rightarrow \\infty$.\n","\n","\n","-   This can be proven through sample based approximations (law of large numbers) of “KL divergences”.\n","\n","\n","-   Mainstay of classical statistics."]},{"cell_type":"markdown","metadata":{"id":"FhmOeKt_Gb7X","colab_type":"text"},"source":["### Probabilistic Interpretation of the Error Function\n","\n","-   Probabilistic Interpretation for Error Function is Negative Log Likelihood.\n","\n","\n","-   *Minimizing* error function is equivalent to *maximizing* log likelihood.\n","\n","\n","-   Maximizing *log likelihood* is equivalent to maximizing the *likelihood* because $\\log$ is monotonic.\n","\n","\n","-   Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to   parameters."]},{"cell_type":"markdown","metadata":{"id":"N8xdFBKwGb7Y","colab_type":"text"},"source":["### Error Function\n","\n","- Remember the expression for the log likelihood function:\n","    \n","    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\frac{1}\n","    {2\\sigma^2}\\sum_{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n","\n","-  The negative log likelihood is the error function \n","\n","    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-         mx_i-c\\right)^{2},$$\n","    \n","   where we have omitted the term $\\frac{n}{2}\\log 2\\pi$ that does not depend on the   parameters.\n","\n","-   Learning proceeds by minimizing this error function for the data\n","    set provided."]},{"cell_type":"markdown","metadata":{"id":"QdRMSHyzGb7Y","colab_type":"text"},"source":["### Connection: Sum of Squares Error\n","\n","-   Ignoring terms which don’t depend on $m$ and $c$ gives\n","    \n","    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n","    \n","    where $f(x_i) = mx_i + c$.\n","\n","\n","-   This is known as the *sum of squares* error function.\n","\n","\n","-   Commonly used and is closely associated with the Gaussian likelihood."]},{"cell_type":"markdown","metadata":{"id":"MlUo_p9nGb7Z","colab_type":"text"},"source":["## Reminder\n","\n","- Two functions involved:\n","\n","  - Prediction function: $f(x_i)$\n","\n","  - Error, or Objective function: $E(m, c)$\n","\n","\n","- Error function depends on parameters through prediction function."]},{"cell_type":"markdown","metadata":{"id":"-w5goibEGb7a","colab_type":"text"},"source":["### Mathematical Interpretation\n","\n","-   What is the mathematical interpretation?\n","\n","    -   There is a cost function.\n","\n","    -   It expresses mismatch between your prediction and reality.\n","        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n","\n","    -   This is known as the sum of squares error."]},{"cell_type":"markdown","metadata":{"id":"Ej4iZmhgGb7b","colab_type":"text"},"source":["### Learning is Optimization\n","\n","-   Learning consists of minimizing the cost function.\n","\n","-   At the minima the gradient is zero.\n","\n","-   Coordinate ascent, find gradient in each coordinate and set to zero.\n","    \\begin{align}\n","     \\frac{\\text{d}E(m)}{\\text{d}m} &= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\\\\\n","                                   0&= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\n","    \\end{align}                               \n"]},{"cell_type":"markdown","metadata":{"id":"7kKObeWkGb7c","colab_type":"text"},"source":["### Learning is Optimization\n","\n","- Fixed point equation for $m$\n","    $$0 =\n","          -2\\sum_{i=1}^n x_iy_i\n","          +2\\sum_{i=1}^n\n","            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n","    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n","          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"]},{"cell_type":"markdown","metadata":{"id":"J-LgjM7eGb7e","colab_type":"text"},"source":["### Learning is Optimization\n","\n","-   Learning consists of minimizing the cost function.\n","\n","-   At the minima the gradient is zero.\n","\n","-   Coordinate ascent, find gradient in each coordinate and set to zero.\n","     $$\\frac{\\text{d}E(c)}{\\text{d}c} = -2\\sum_{i=1}^n \\left(y_i- m x_i - c \\right)$$\n","    \n","     $$0 = -2\\sum_{i=1}^n\\left(y_i-m x_i - c \\right)$$\n","    "]},{"cell_type":"markdown","metadata":{"id":"7vO8InexGb7f","colab_type":"text"},"source":["### Learning is Optimization\n","\n","- Fixed point equation for $c$\n","    \\begin{align}\n","        0 &= -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c\\\\\n","        c &= \\frac{\\sum_{i=1}^n \\left(y_i-mx_i\\right)}{n}\n","    \\end{align}    "]},{"cell_type":"markdown","metadata":{"id":"i_-iC4ufGb7f","colab_type":"text"},"source":["### Fixed Point Updates\n","\n","Worked example. \n","   \n","   \\begin{align}\n","       c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n","       m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n","      \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n","  \\end{align}"]},{"cell_type":"markdown","metadata":{"id":"r5f46EZOGb7g","colab_type":"text"},"source":["### Important Concepts Not Covered\n","\n","-   Other optimization methods:\n","\n","    -   Second order methods, conjugate gradient, quasi-Newton\n","        and Newton.\n","\n","    -   Effective heuristics such as momentum.\n","\n","-   Local vs global solutions."]},{"cell_type":"markdown","metadata":{"id":"o7v9W6F3Gb7h","colab_type":"text"},"source":["### Reading\n","\n","- Section 1.1-1.2 of Rogers and Girolami (2016) for fitting linear models. \n","- Section 1.2.5 of Bishop (2006) up to equation 1.65.\n"]},{"cell_type":"markdown","metadata":{"id":"sPxxlw3HGb7h","colab_type":"text"},"source":["### Multi-dimensional Inputs\n","\n","-   Multivariate functions involve more than one input.\n","\n","-   Height might be a function of weight and gender.\n","\n","-   There could be other contributory factors.\n","\n","-   Place these factors in a feature vector $\\mathbf{x}_i$.\n","\n","-   Linear function is now defined as\n","    $$f(\\mathbf{x}_i) = \\sum_{j=1}^D w_j x_{i, j} + c$$"]},{"cell_type":"markdown","metadata":{"id":"x3fa-rlcGb7i","colab_type":"text"},"source":["### Vector Notation\n","\n","-   Write in vector notation,\n","    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n","    \n","    \n","\n","-   Can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n","    which is always 1.\n","    \n","    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"]},{"cell_type":"markdown","metadata":{"id":"b1u3QfK1Gb7i","colab_type":"text"},"source":["### Log Likelihood for Multivariate Regression\n","\n","-   The likelihood of a single data point is\n","    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n","        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n","\n","-   Leading to a log likelihood for the data set of\n","    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n","          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n","            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n","\n","-   And a corresponding error function of\n","    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n","          \\sigma^2 + \\frac{\\sum\n","            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"]},{"cell_type":"markdown","metadata":{"id":"yz8Abj1iGb7j","colab_type":"text"},"source":["### Expand the Brackets\n","\n","\\begin{align*}\n","  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i- \n","                               \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\\\\n","                          = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum           \n","                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum \n","                              _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i+\\frac{1}\n","                              {2\\sigma^2}\\sum_{i=1}^{n}                            \n","                              \\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w} \n","                             +\\text{const}.\\\\\n","                         = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n","                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n","                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n","                             +\\frac{1}{2\\sigma^2} \n","                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n","                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n","\\end{align*}"]},{"cell_type":"markdown","metadata":{"id":"vhjg078CGb7j","colab_type":"text"},"source":["### Multivariate Derivatives\n","\n","-   We will need some multivariate calculus.\n","\n","\n","-   For now some simple multivariate differentiation:\n","    $$\\frac{\\text{d}{\\mathbf{w}^{\\top}}{\\mathbf{a}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n","    and\n","    $$\\frac{\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n","    or if $\\mathbf{A}$ is symmetric (*i.e.*\n","    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n","    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$"]},{"cell_type":"markdown","metadata":{"id":"Up53naIyGb7k","colab_type":"text"},"source":["### Differentiate\n","\n","The objective function is given as\n","\n","\\begin{align*}\n","  E(\\mathbf{w},\\sigma^2)  = \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n","                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n","                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n","                             +\\frac{1}{2\\sigma^2} \n","                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n","                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n","\\end{align*}\n","\n","Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n","\n","$$\\frac{\\partial E\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i-\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n","\n","Leading to\n","$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i,$$\n","\n","Using matrix notation, it can be shown that:\n","$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}\\qquad \\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"]},{"cell_type":"markdown","metadata":{"id":"5y6oFcjIGb7k","colab_type":"text"},"source":["### $\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top=\\mathbf{X}^{\\top}\\mathbf{X}$\n","\n","Let us write \n","\n","\\begin{align*}\n","    \\mathbf{X} =\n","                \\begin{bmatrix}\n","                    \\mathbf{x}^{\\top}_1\\\\\n","                    \\mathbf{x}^{\\top}_2\\\\\n","                    \\vdots\\\\\n","                    \\mathbf{x}^{\\top}_n\n","                \\end{bmatrix},\n","     \\quad\n","     \\mathbf{X}^{\\top} =\n","                \\begin{bmatrix}\n","                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n","                \\end{bmatrix}.\n","\\end{align*}\n","\n","We can then say that\n","\n","\\begin{align*}\n","    \\mathbf{X}^{\\top}\\mathbf{X} =\n","                \\begin{bmatrix}\n","                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n","                \\end{bmatrix} \n","                \\begin{bmatrix}\n","                    \\mathbf{x}^{\\top}_1\\\\\n","                    \\mathbf{x}^{\\top}_2\\\\\n","                    \\vdots\\\\\n","                    \\mathbf{x}^{\\top}_n\n","                \\end{bmatrix} =  \n","                \\mathbf{x}_1\\mathbf{x}^{\\top}_1+\\mathbf{x}_2\\mathbf{x}^{\\top}_2+\\cdots+\\mathbf{x}_n\n","                \\mathbf{x}^{\\top}_n = \\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top.\n","\\end{align*}\n"]},{"cell_type":"markdown","metadata":{"id":"J1H-o181Gb7l","colab_type":"text"},"source":["### Update Equations\n","\n","-   Update for $\\mathbf{w}^{*}$.\n","    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n","\n","-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n","    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"]},{"cell_type":"markdown","metadata":{"id":"Bxz8kaiNGb7l","colab_type":"text"},"source":["### Reading\n","\n","- Section 1.3 of Rogers and Girolami (2016) for Matrix & Vector Review.\n"," "]}]}